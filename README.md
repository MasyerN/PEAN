Abstract -- A crucial asset for deep learning in Whole Slide Image (WSI) diagnosis is pathologists’ expertise, which is commonly used in fully supervised learning through pixel-wise manual annotations. Although this method achieves higher performance than weakly supervised learning solely trained based on reported diagnostic results, the significantly manual labor of pathologists hampers efficacy to train fully supervised deep learning models. To efficiently leverage expertise of pathologists, achieve higher auxiliary diagnosis and reduce labor costs, we collected pathologists’ visual attention patterns captured by eye-tracking device to replace manual annotations. Subsequently, a Pathology Expertise Acquisition Network (PEAN) is designed, which extracts pathologists’ expertise from their visual patterns and leverages the expertise to identify subregions with high diagnostic value. In this study, 5,881 WSIs of skin lesion diagnosis are collected. Among them, 1,565 have both pathologists’ visual patterns and manual annotations for comparison. First, while comparing to manual annotation, collection of pathologists’ eye-tracking data reduces the average working time of pathologists from 14.2 minutes to 36.5 seconds. Second, driven by the pathological expertise extracted by PEAN, it achieved the best performance in the WSI five-classification tasks, with AUC of 0.984 and accuracy of 93.1%. Third, the WSI-reviewing behavior of pathologists was replicated, showing consistency with the pathologists themselves and statistically improving the diagnostic performance of existing deep learning models (p=0.005). Thus, this novel approach can replace labor-intensive manual annotations and lead to more accurate assisted diagnosis, which holds promise for future large-scale research applications.


We have provided a download link for the dataset used in this study: (https://pan.baidu.com/s/1g_9mC1Q8_BQuySvVBSK5cA?pwd=4fuj  4fuj), which includes data for 150 cases (Whole Slide Images, slide reading data, and manual annotations). Additional data can be requested from the authors for research purposes. Due to the constraints of the double-blind review process, the application channel will be opened after the blind review phase concludes. For the structure and reading of EPR files (pathologist's slide-reviewing data), please refer to /EPR_file.xlsx and /Get_expertise/eprRead.py


The model PEAN proposed in this study is carried out in multiple steps: 1. Extracting the experience of pathologists; 2. Build a classification model; 3. Imitate the behavior of pathologists.

Environment description of Step1: Nvidia A6000 GPU with Python=3.7.12 and cuda=11.1. torch=1.8.0. torchvision=0.9.0.  The remaining environmental dependencies refer to GCMAE (https://github.com/StarUniversus/gcmae). It is worth noting that this environment may cause conflicts in the torch version. In order to address this conflict, we have chosen to modify the torch source code. Please query /Get_expertise/__six.py

Regarding the initial step of extracting pathologists' expertise, please execute /Get_expertise/IRL.py. Prior to execution, ensure the paths for the raw data and the model storage are correctly specified at lines 30 and 326, respectively. The raw data consists of a dictionary saved by PyTorch: {'train': [['ndpi': xx, 'epr': xx], ...], 'test': [['ndpi': xx, 'epr': xx], ...]}. Herein, 'ndpi' represents the path to the Whole Slide Images (WSI), and 'epr' denotes the corresponding slide reading data.

In this step, we used a pre trained encoder: GCMAE (https://github.com/StarUniversus/gcmae). We have uploaded the encoder weights and can modify them in/Get_expertise/maeModel.py to customize the encoder. (https://pan.baidu.com/s/1jUoXKi5xCaKl53dpxjMqzg?pwd=asai, asai) 

In the second step, we developed PEAN-C using pathologist experience for feature distillation based on Transformer. Please refer to/PAN/req.txt for the environmental dependencies of the second step. （https://github.com/szc19990412/TransMIL）

In the third step, we developed PEAN-I based on reinforcement learning, which can mimic the pathologist's slide-reviewing behavior. The environmental dependency in the third step is the same as in the first step.
