Abstract -- Based on the expertise of pathologists, the pixelwise manual annotation has provided substantial support for training deep learning models of whole slide images (WSI)-assisted diagnostic. However, the collection of pixelwise annotation demands massive annotation time from pathologists, leading to a high burden of  medical manpower resources, hindering to construct larger datasets and more precise diagnostic models. To obtain pathologists’ expertise with minimal pathologist workloads then achieve precise diagnostic, we collected the image review patterns of pathologists by eye-tracking devices. Simultaneously, we designed a deep learning system: Pathology Expertise Acquisition Network (PEAN), based on the collected visual patterns, which can decode pathologists’ expertise then diagnoses WSIs. Eye-trackers reduced the time required for annotating WSIs to 4%, of the manual annotation. We evaluated PEAN on 5,881 WSIs and 5 categories of skin lesions, achieving a high AUC of 0.992 and an accuracy of 96.3% on diagnostic prediction.  This study fills the gap in existing models’ inability to learn from the diagnostic processes of pathologists. Its efficient data annotation and precise diagnostics provide assistance in both large-scale data collection and clinical care.


We have provided a download link for the dataset used in this study: (https://pan.baidu.com/s/1g_9mC1Q8_BQuySvVBSK5cA?pwd=4fuj  4fuj), which includes data for 150 cases (Whole Slide Images, slide reading data, and manual annotations). Additional data can be requested from the authors for research purposes. Due to the constraints of the double-blind review process, the application channel will be opened after the blind review phase concludes. For the structure and reading of EPR files (pathologist's slide-reviewing data), please refer to /EPR_file.xlsx and /Get_expertise/eprRead.py


The model PEAN proposed in this study is carried out in multiple steps: 1. Extracting the experience of pathologists; 2. Build a classification model; 3. Imitate the behavior of pathologists.

Environment description of Step1: Nvidia A6000 GPU with Python=3.7.12 and cuda=11.1. torch=1.8.0. torchvision=0.9.0.  The remaining environmental dependencies refer to GCMAE (https://github.com/StarUniversus/gcmae). It is worth noting that this environment may cause conflicts in the torch version. In order to address this conflict, we have chosen to modify the torch source code. Please query /Get_expertise/__six.py

Regarding the initial step of extracting pathologists' expertise, please execute /Get_expertise/IRL.py. Prior to execution, ensure the paths for the raw data and the model storage are correctly specified at lines 30 and 326, respectively. The raw data consists of a dictionary saved by PyTorch: {'train': [['ndpi': xx, 'epr': xx], ...], 'test': [['ndpi': xx, 'epr': xx], ...]}. Herein, 'ndpi' represents the path to the Whole Slide Images (WSI), and 'epr' denotes the corresponding slide reading data.

In this step, we used a pre trained encoder: GCMAE (https://github.com/StarUniversus/gcmae). We have uploaded the encoder weights and can modify them in/Get_expertise/maeModel.py to customize the encoder. (https://pan.baidu.com/s/1jUoXKi5xCaKl53dpxjMqzg?pwd=asai, asai) 

In the second step, we developed PEAN-C using pathologist experience for feature distillation based on Transformer. Please refer to/PAN/req.txt for the environmental dependencies of the second step. （https://github.com/szc19990412/TransMIL）

In the third step, we developed PEAN-I based on reinforcement learning, which can mimic the pathologist's slide-reviewing behavior. The environmental dependency in the third step is the same as in the first step.

Installing the code on a computer with GPU-A6000 usually takes 10 minutes, and running an instance usually takes 1 minute.
